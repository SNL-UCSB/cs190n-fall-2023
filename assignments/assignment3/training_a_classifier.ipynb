{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44fd7f",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "In this assignment you will be training a simple classifier for detecting the type of video service being used and understand how the model is doing the classification.  \n",
    "\n",
    "## Due Date: December 8th\n",
    "Please submit a PDF verion of this notebook with all the cells evaluated and answers displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46511469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustee import ClassificationTrustee\n",
    "import matplotlib.pyplot as plt\n",
    "from scapy.all import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import binascii\n",
    "import ipaddress\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Display all rows for the DataFrame\n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533dd458",
   "metadata": {},
   "source": [
    "### Task 0\n",
    "In discussion section we used CIC Flowmeter to take care of preprocessing our packet traces. In this assignment we will use CSVs created using the following tshark script.\n",
    "\n",
    "```\n",
    "tshark -r <pcap_name> -T fields -E separator=/t -e frame.time_epoch -e ip.src -e tcp.srcport -e udp.srcport -e ip.dst -e tcp.dstport -e udp.dstport -e ip.len -e ip.hdr_len -e ip.proto -e tcp.flags -e tcp.seq_raw -e tcp.ack_raw -e tcp.hdr_len -e udp.length -e tcp.analysis.retransmission >> <output_filepath>.csv\n",
    "```\n",
    "\n",
    "Therefore the columns in each CSV are as follows:\n",
    "- Frame Timestamp (`frame.time_epoch`)\n",
    "- Source IP Address (`ip.src`)\n",
    "- TCP Source Port (`tcp.srcport`)\n",
    "- UDP Source Port (`udp.srcport`)\n",
    "- Destination IP Address (`ip.dst`)\n",
    "- TCP Destination Port (`tcp.dstport`)\n",
    "- UDP Destination Port (`udp.dstport`)\n",
    "- IP Packet Length (`ip.len`)\n",
    "- IP Header Length (`ip.hdr_len`)\n",
    "- Transport Protocol (`ip.proto`)\n",
    "- TCP Flags (`tcp.flags`)\n",
    "- TCP Sequence Number (`tcp.seq_raw`)\n",
    "- TCP Acknowledgement Number (`tcp.ack_raw`)\n",
    "- TCP Header Length (`tcp.hdr_len`)\n",
    "- UDP Length (`udp.length`)\n",
    "- TCP Retransmission (`tcp.analysis.retransmission`)  \n",
    "\n",
    "You can find the CSV files for Twitch and Vimeo respectively on the lab server at the paths:  \n",
    "```/mnt/cs190n/assignment3/twitch_csvs/```  \n",
    "```/mnt/cs190n/assignment3/vimeo_csvs/```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bca676",
   "metadata": {},
   "outputs": [],
   "source": [
    "vimeo_csvs = [\n",
    "    '/mnt/md0/cs190n/assignment3/vimeo_csvs/vimeo_capture1.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/vimeo_csvs/vimeo_capture2.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/vimeo_csvs/vimeo_capture3.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/vimeo_csvs/vimeo_capture4.csv',    \n",
    "]\n",
    "\n",
    "twitch_csvs = [\n",
    "    '/mnt/md0/cs190n/assignment3/twitch_csvs/twitch_capture1.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/twitch_csvs/twitch_capture2.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/twitch_csvs/twitch_capture3.csv',\n",
    "    '/mnt/md0/cs190n/assignment3/twitch_csvs/twitch_capture4.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_to_int(ip):\n",
    "    try:\n",
    "        return int(ipaddress.ip_address(ip))\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686007c",
   "metadata": {},
   "source": [
    "#### Pre-process Vimeo Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cecd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vimeo_packets = pd.DataFrame()\n",
    "for v in vimeo_csvs:\n",
    "    \n",
    "    # Read in CSV\n",
    "    curr_df = pd.read_csv(v,\n",
    "                          sep='\\t',\n",
    "                          header=None,\n",
    "                          dtype = {0: 'float',\n",
    "                                   1: 'string',\n",
    "                                   2: 'Int64',\n",
    "                                   3: 'Int64',\n",
    "                                   4: 'string',\n",
    "                                   5: 'Int64',\n",
    "                                   6: 'Int64',\n",
    "                                   7: 'string',\n",
    "                                   8: 'string',\n",
    "                                   9: 'string',\n",
    "                                   10: 'string',\n",
    "                                   11: 'Int64',\n",
    "                                   12: 'Int64',\n",
    "                                   13: 'Int64',\n",
    "                                   14: 'Int64',\n",
    "                                   15: 'Int64' \n",
    "                                   })\n",
    "    \n",
    "    curr_df = curr_df.rename(columns={0: 'frame_timestamp',\n",
    "                                      1: 'ip_src',\n",
    "                                      2: 'tcp_srcport',\n",
    "                                      3: 'udp_srcport',\n",
    "                                      4: 'ip_dst',\n",
    "                                      5: 'tcp_dstport',\n",
    "                                      6: 'udp_dstport',\n",
    "                                      7: 'ip_len',\n",
    "                                      8: 'ip_header_len',\n",
    "                                      9: 'protocol',\n",
    "                                      10: 'tcp_flags',\n",
    "                                      11: 'tcp_seq_num',\n",
    "                                      12: 'tcp_ack_num',\n",
    "                                      13: 'tcp_header_len',\n",
    "                                      14: 'udp_len',\n",
    "                                      15: 'tcp_retransmission',\n",
    "                                     })\n",
    "    \n",
    "    # Get basename of file\n",
    "    basename = v.split('/')[-1].split('.')[0]\n",
    "    curr_df['file'] = basename\n",
    "    \n",
    "    # Convert IPs to integers\n",
    "    curr_df['ip_src_int'] = curr_df['ip_src'].apply(ip_to_int)\n",
    "    curr_df['ip_dst_int'] = curr_df['ip_dst'].apply(ip_to_int)\n",
    "    \n",
    "    \n",
    "    curr_df = curr_df.drop(columns=['ip_src', 'ip_dst']).rename(columns={'ip_src_int': 'ip_src',\n",
    "                                                                         'ip_dst_int': 'ip_dst'})\n",
    "    \n",
    "    # Append packets to current DF\n",
    "    vimeo_packets = pd.concat([vimeo_packets, curr_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccc3c5",
   "metadata": {},
   "source": [
    "#### Pre-process Twitch Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch_packets = pd.DataFrame()\n",
    "for v in twitch_csvs:\n",
    "    \n",
    "    # Read in CSV\n",
    "    curr_df = pd.read_csv(v,\n",
    "                          sep='\\t',\n",
    "                          header=None,\n",
    "                          dtype = {0: 'float',\n",
    "                                   1: 'string',\n",
    "                                   2: 'Int64',\n",
    "                                   3: 'Int64',\n",
    "                                   4: 'string',\n",
    "                                   5: 'Int64',\n",
    "                                   6: 'Int64',\n",
    "                                   7: 'string',\n",
    "                                   8: 'string',\n",
    "                                   9: 'string',\n",
    "                                   10: 'string',\n",
    "                                   11: 'Int64',\n",
    "                                   12: 'Int64',\n",
    "                                   13: 'Int64',\n",
    "                                   14: 'Int64',\n",
    "                                   15: 'Int64' \n",
    "                                   })\n",
    "    \n",
    "    curr_df = curr_df.rename(columns={0: 'frame_timestamp',\n",
    "                                      1: 'ip_src',\n",
    "                                      2: 'tcp_srcport',\n",
    "                                      3: 'udp_srcport',\n",
    "                                      4: 'ip_dst',\n",
    "                                      5: 'tcp_dstport',\n",
    "                                      6: 'udp_dstport',\n",
    "                                      7: 'ip_len',\n",
    "                                      8: 'ip_header_len',\n",
    "                                      9: 'protocol',\n",
    "                                      10: 'tcp_flags',\n",
    "                                      11: 'tcp_seq_num',\n",
    "                                      12: 'tcp_ack_num',\n",
    "                                      13: 'tcp_header_len',\n",
    "                                      14: 'udp_len',\n",
    "                                      15: 'tcp_retransmission',\n",
    "                                     })\n",
    "    \n",
    "    \n",
    "    # Get basename of file\n",
    "    basename = v.split('/')[-1].split('.')[0]\n",
    "    curr_df['file'] = basename\n",
    "    \n",
    "    # Convert IPs to integers\n",
    "    curr_df['ip_src_int'] = curr_df['ip_src'].apply(ip_to_int)\n",
    "    curr_df['ip_dst_int'] = curr_df['ip_dst'].apply(ip_to_int)\n",
    "    \n",
    "    \n",
    "    curr_df = curr_df.drop(columns=['ip_src', 'ip_dst']).rename(columns={'ip_src_int': 'ip_src',\n",
    "                                                                         'ip_dst_int': 'ip_dst'})\n",
    "    \n",
    "    # Append packets to current DF\n",
    "    twitch_packets = pd.concat([twitch_packets, curr_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1668a",
   "metadata": {},
   "source": [
    "Before moving on to the tasks for this assignment make sure you have preprocessed the twitch and vimeo pcaps. It's okay if there are some '<NA>' values in your dataframe. The dtype parameter specifies the datatype of the values in that column. Note that the `ip_len`, `ip_header_len` and `protocol` fields are string values but we can cast in to an integer using `astype('Int64')`, and the src and destination IP are represented as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vimeo_packets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitch_packets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4e219",
   "metadata": {},
   "source": [
    "### Task 1: Preprocess the Data\n",
    "In this task we will be preprocessing our data to label as either a 'twitch' or a 'vimeo' for training our classifier. We will need to discard background traffic in our captures that does not represent Twitch or Vimeo traffic. We will use a simple methodology to identify relevant flows for Twitch and Vimeo. We will only consider TCP flows with at least 30 inbound AND outbound packets with a TCP length greater than 0 within the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e37c7",
   "metadata": {},
   "source": [
    "#### Task 1a\n",
    "The client IP in these packet captures is 172.17.0.2, discard the background traffic and retain only the packets with a source or destination IP equal to 172.17.0.2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fe841",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_packets = pd.concat([vimeo_packets, twitch_packets], ignore_index=True)\n",
    "\n",
    "# TODO: Filter for packets that have a source IP or destination equal to 172.17.0.2\n",
    "client_condition = # <fill me in>\n",
    "\n",
    "all_packets = all_packets[client_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479b368",
   "metadata": {},
   "source": [
    "#### Task 1b\n",
    "Now we will filter these dataframes to only include TCP traffic (protocol 6), and calculate the TCP payload length. This new column will be called `tcp_len` which represents the length of the TCP payload. This is important because it helps us differentiate packets carrying data from the acknowledgement packets. The TCP length can be calculated as the difference between the IP Packet Length and the sum of the IP Header Length and TCP Header Length.    \n",
    "```TCP_Payload_Length = IP_Packet_Length - (IP_Header_Length + TCP_Header_Length)```    \n",
    "Be sure to note the datatypes of each column specified above to take care of any necessary string to Int64 conversions. Remember you can cast a string to an integer using `df['column_name'].astype('Int64')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter for packets that have a protocol of TCP (6)\n",
    "protocol_condition = # <fill me in>\n",
    "\n",
    "all_packets = all_packets[protocol_condition]\n",
    "\n",
    "# TODO: Calculate TCP payload length for each packet\n",
    "all_packets['tcp_len'] = # <fill me in>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc9ff9",
   "metadata": {},
   "source": [
    "#### Task 1c\n",
    "We will only consider TCP flows with at least 30 inbound OR outbound packets with a TCP length greater than 0 within the flow and discard packets from the remaining flows.    \n",
    "Remember a flow is a unique 5-tuple identifier consisting of Source IP, Source Port, Destination IP, Destination Port, and Protocol.   \n",
    "Be sure to note the datatypes of each column specified above to take care of any necessary string to int conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38737296",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flows = (all_packets[all_packets['tcp_len'] > 0]\n",
    "             .groupby(['protocol', 'ip_src', 'tcp_srcport', 'ip_dst', 'tcp_dstport'], dropna=False)\n",
    "             .agg({'frame_timestamp': 'count'})\n",
    "             .rename(columns = {'frame_timestamp':'pkt_count'}))\n",
    "all_flows = all_flows.reset_index()\n",
    "\n",
    "# TODO: filter for flows with at least 30 packets per flow\n",
    "pkt_count_condition = # <fill me in>\n",
    "all_flows = all_flows[pkt_count_condition]\n",
    "\n",
    "all_flows = all_flows[['protocol', 'ip_src', 'tcp_srcport', 'ip_dst', 'tcp_dstport']]\n",
    "all_flows_list = list(all_flows.itertuples(index=False, name=None))\n",
    "\n",
    "all_packets['5tuple'] = list(zip(all_packets['protocol'], all_packets['ip_src'], all_packets['tcp_srcport'], all_packets['ip_dst'], all_packets['tcp_dstport']))\n",
    "\n",
    "# TODO: filter for flows that are in all_flows_list using the 5tuple key\n",
    "flow_condition = # <fill me in>\n",
    "all_packets = all_packets[flow_condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915c491",
   "metadata": {},
   "source": [
    "### Task 2: Feature Selection\n",
    "In this task we will be computing the features that we want to use as input to our model. We will choose the following features for our model, (grouped by 5-tuple):\n",
    "- Total packets\n",
    "- Total bytes in TCP payload\n",
    "- Inter-packet delay (avg)\n",
    "- Inter-packet delay (max)\n",
    "- Inter-packet delay (stddev)\n",
    "- Direction (0 for outbound, 1 for inbound)\n",
    "\n",
    "Compute these features for each flow. You'll want to use the groupby function to group each flow and the agg function to compute these metrics aggregated over each group. We've provided you with the functions to use to calculate inter-packet delay, and you should be able to use `count` and `sum` to compute the total packets and bytes per flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f58778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_packet_delay_avg(series):\n",
    "    return (series.diff()).mean()\n",
    "\n",
    "def inter_packet_delay_stddev(series):\n",
    "    return (series.diff()).std()\n",
    "\n",
    "def inter_packet_delay_max(series):\n",
    "    return (series.diff()).max()\n",
    "\n",
    "# TODO: Group the packets by 5-tuple and compute the features using the agg function\n",
    "all_flow_stats = # <fill me in>\n",
    "\n",
    "# TODO: Label the direction of packets, 0 for outbound and 1 for inbound\n",
    "all_flow_stats['direction'] = #TODO: <fill me in>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de0f55",
   "metadata": {},
   "source": [
    "### Task 3: Labelling the Data\n",
    "Now we will label our flows as twitch or vimeo. We included the `file` column above which has a label for the specific file the packet is from. You can use this field to label each flow as twitch or vimeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Label the flows as twitch or vimeo\n",
    "all_flows_stats['Label'] = # <fill me in>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1081f9",
   "metadata": {},
   "source": [
    "### Task 4: Training a Model\n",
    "Now that we have selected our features and labelled our data, we can train a simple classifier. Choose a classifier from [python-scikit](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41930729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows with NaN\n",
    "all_flow_stats = all_flow_stats.dropna()\n",
    "\n",
    "# separate the data frame to features and answers\n",
    "target_variable = 'Label'\n",
    "if 'file' in set(all_flow_stats.columns):\n",
    "    train_features = list(set(all_flow_stats.columns) - {target_variable} - {'file'})\n",
    "else:\n",
    "    train_features = list(set(all_flow_stats.columns) - {target_variable})\n",
    "x_train = all_flow_stats[train_features]\n",
    "y_train = all_flow_stats[target_variable]\n",
    "\n",
    "# TODO: define the classifier you want to test out\n",
    "clf = # <fill me in>\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Create a set of predictions based on our model and view the precision / recall\n",
    "y_pred = clf.predict(x_train.values)\n",
    "print(metrics.classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa407c",
   "metadata": {},
   "source": [
    "### Task 5: Understanding the Model\n",
    "You will likely observe a very high if not perfect precision/recall because we are using a small amount of data and we are training / testing using the same dataset. This means that we were able to fit a model that can classify all of our training data correctly. Now we will generate a report from Trustee as well as visualize our model to understand which features are being used to classify our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd1b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create and Train a Trustee Tree\n",
    "trustee = ClassificationTrustee(expert=clf)\n",
    "trustee.fit(x_train, y_train, num_samples=len(x_train) // 2, num_iter=20, train_size=0.99)\n",
    "\n",
    "# Display Trustee Results\n",
    "_, dt, _, score = trustee.explain()\n",
    "print(f\"Training score of pruned DT: {score}\")\n",
    "dt_y_pred = dt.predict(x_train)\n",
    "print(\"Model explanation global fidelity report:\")\n",
    "print(metrics.classification_report(clf.predict(x_train), dt_y_pred))\n",
    "print(\"Model explanation score report:\")\n",
    "print(metrics.classification_report(y_train, dt_y_pred))\n",
    "\n",
    "# plot a tree\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "plot_tree(dt, feature_names=x_train.columns, class_names=sorted(all_flow_stats['Label'].unique()), filled=True, max_depth=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34595ec",
   "metadata": {},
   "source": [
    "Given the report from Trustee, what feature(s) do you think are most importantly used by this classifier to predict whether a flow is Vimeo or Twitch? Is this an instance of shortcut learning? Try re-training the model without including the 5-tuple for the flow as part of the list of features and observe how the decision tree changes. Why might it a bad idea to include IPs as features into this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0401cfe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
